{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTrTKXr3vkg9OL53RJLb+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-caballero/Data-Discovery/blob/main/modulo3_manipulacion_datos_faltantes/Handling_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://posgrado.utec.edu.pe/sites/default/files/2023-08/Testimonial-home-2.jpg\" alt=\"HTML5 Icon\" width=\"900\" height=\"250\" >\n"
      ],
      "metadata": {
        "id": "63Z52XtzaaqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Missing Data**"
      ],
      "metadata": {
        "id": "nXzKdHNnalF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivos**\n",
        "\n",
        "- Aplicar distintas técnicas de imputación:\n",
        "\n",
        "  - Medidas de tendencia central (media, mediana, moda)\n",
        "\n",
        "  - Imputación por regresión\n",
        "\n",
        "  - KNN imputation\n",
        "\n",
        "  - XGBoost imputador\n",
        "\n",
        "  - Autoencoders\n",
        "\n",
        "  - MICE (Multiple Imputation by Chained Equations)\n",
        "\n",
        "- Entrenar un modelo base (RandomForest o XGBoost) para comparar el impacto de cada estrategia de imputación en términos de performance (accuracy, AUC, etc.)\n",
        "\n",
        "- Reflexionar sobre las implicancias de cada método para modelos analíticos."
      ],
      "metadata": {
        "id": "l3bhQt_IalxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "Loan Prediction - train.csv\n",
        "\n",
        "Puedes obtenerlo desde:\n",
        " https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii"
      ],
      "metadata": {
        "id": "Inh96ly5azO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Librerías necesarias"
      ],
      "metadata": {
        "id": "rvo1B-sea4nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy seaborn matplotlib scikit-learn xgboost fancyimpute tensorflow missingno\n"
      ],
      "metadata": {
        "id": "RPK6gi4ka6Xj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Carga y diagnóstico inicial"
      ],
      "metadata": {
        "id": "wOP8Iso2a8uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df.drop(\"Loan_ID\", axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "8katKHaya_1y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Medidas de tendencia central (media, mediana, moda)"
      ],
      "metadata": {
        "id": "LodYmLpGbCZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80twajk3aaHM"
      },
      "outputs": [],
      "source": [
        "df_mean = df.copy()\n",
        "\n",
        "df_mean[\"LoanAmount\"].fillna(df_mean[\"LoanAmount\"].mean(), inplace=True)\n",
        "df_mean[\"Gender\"].fillna(df_mean[\"Gender\"].mode()[0], inplace=True)\n",
        "df_mean[\"Self_Employed\"].fillna(df_mean[\"Self_Employed\"].mode()[0], inplace=True)\n",
        "df_mean[\"Dependents\"].fillna(df_mean[\"Dependents\"].mode()[0], inplace=True)\n",
        "df_mean[\"Credit_History\"].fillna(df_mean[\"Credit_History\"].mode()[0], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Imputación por regresión lineal"
      ],
      "metadata": {
        "id": "wQLWxMqmbK1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el modelo de regresión lineal de scikit-learn\n"
      ],
      "metadata": {
        "id": "WW6LW43exv_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "LJi0IeN-xFG8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una copia del dataframe original para no modificarlo directamente\n"
      ],
      "metadata": {
        "id": "8NVRcys0xsSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg = df.copy()"
      ],
      "metadata": {
        "id": "7BZ1G8k0bODa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos las filas donde \"LoanAmount\" NO es nulo (estos datos los usaremos para entrenar el modelo)\n",
        "train = df_reg[df_reg[\"LoanAmount\"].notnull()]\n",
        "\n",
        "# Separamos las filas donde \"LoanAmount\" ES nulo (estos datos se imputarán)\n",
        "test = df_reg[df_reg[\"LoanAmount\"].isnull()]"
      ],
      "metadata": {
        "id": "rChdid-JxTL2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionamos como variables predictoras el ingreso del solicitante y del co-solicitante para el conjunto de entrenamiento\n"
      ],
      "metadata": {
        "id": "nnogvSmVxqFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train[[\"ApplicantIncome\", \"CoapplicantIncome\"]]\n",
        "\n",
        "# La variable objetivo a predecir es \"LoanAmount\"\n",
        "y_train = train[\"LoanAmount\"]\n",
        "\n",
        "# Para el conjunto de prueba (filas con \"LoanAmount\" nulo), también seleccionamos las variables predictoras\n",
        "X_test = test[[\"ApplicantIncome\", \"CoapplicantIncome\"]]\n"
      ],
      "metadata": {
        "id": "jaB0Z5fLxU40"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos y entrenamos el modelo de regresión lineal"
      ],
      "metadata": {
        "id": "fGvwNdi5xiRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "36XKfdsLxemv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " el modelo entrenado para predecir los valores faltantes de \"LoanAmount\"\n"
      ],
      "metadata": {
        "id": "v2Q8Eqs2xk9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "5aMTe4AWxfxa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asignamos los valores imputados (predichos) a las posiciones originales donde \"LoanAmount\" era nulo\n"
      ],
      "metadata": {
        "id": "QXaRHlRzxnHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg.loc[df_reg[\"LoanAmount\"].isnull(), \"LoanAmount\"] = preds\n"
      ],
      "metadata": {
        "id": "wfObMr8DxgpH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Imputación KNN"
      ],
      "metadata": {
        "id": "YFnm0CuwbQi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el imputador KNN de scikit-learn\n",
        "y LabelEncoder para convertir variables categóricas a numéricas\n",
        "\n"
      ],
      "metadata": {
        "id": "Phowr3P6y10D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "RCsES-Y3y0V0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una copia del dataframe original para no modificarlo directamente\n"
      ],
      "metadata": {
        "id": "PNQsTaJmzIDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_knn = df.copy()"
      ],
      "metadata": {
        "id": "IZtvpYFybUN5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos LabelEncoder a todas las columnas del dataframe.\n",
        "# Esto convierte variables categóricas (strings) en valores numéricos.\n",
        "# Nota: KNNImputer solo funciona con variables numéricas.\n",
        "df_knn = df_knn.apply(LabelEncoder().fit_transform)"
      ],
      "metadata": {
        "id": "KFpby7TKzQdT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una instancia del imputador KNN\n",
        "# n_neighbors=30 indica que se usarán los 30 registros más cercanos (similares)\n",
        "knn_imputer = KNNImputer(n_neighbors=30)"
      ],
      "metadata": {
        "id": "fhCpWXn6zTUP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos el imputador al dataframe y reconstruimos el DataFrame imputado\n",
        "# fit_transform encuentra los valores faltantes y los imputa usando los vecinos más cercanos\n",
        "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_knn), columns=df_knn.columns)"
      ],
      "metadata": {
        "id": "kqEaA7tXzT1Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Imputación con XGBoost (modelo para imputar)"
      ],
      "metadata": {
        "id": "6BS0jGpibWTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos XGBoost, un modelo de gradient boosting muy potente para tareas de regresión y clasificación\n",
        "import xgboost as xgb\n",
        "\n"
      ],
      "metadata": {
        "id": "UunQbsprzjHM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_xgb = df.copy()"
      ],
      "metadata": {
        "id": "hDf0-ugmzj5K"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Definimos una función para imputar una columna con valores faltantes usando un modelo XGBoost Regressor\n",
        "def xgb_impute(df, target_col):\n",
        "    # 1. Separar el conjunto de entrenamiento (valores NO nulos) y prueba (valores nulos) para la variable objetivo\n",
        "    train = df[df[target_col].notnull()]\n",
        "    test = df[df[target_col].isnull()]\n",
        "\n",
        "    # 2. Seleccionar las variables predictoras: excluye la columna objetivo y las columnas completamente vacías\n",
        "    features = [col for col in df.columns if col != target_col and df[col].notnull().sum() > 0]\n",
        "\n",
        "    # 3. Elimina registros del entrenamiento que aún tienen missing en las variables predictoras seleccionadas\n",
        "    train = train.dropna(subset=features)\n",
        "\n",
        "    # 4. Convierte variables categóricas a dummies (One-Hot Encoding) para entrenamiento\n",
        "    X_train = pd.get_dummies(train[features])\n",
        "\n",
        "    # 5. Define la variable objetivo (lo que vamos a imputar)\n",
        "    y_train = train[target_col]\n",
        "\n",
        "    # 6. Convierte también el conjunto de prueba a dummies\n",
        "    X_test = pd.get_dummies(test[features])\n",
        "\n",
        "    # 7. Asegura que X_test tenga las mismas columnas que X_train (rellena columnas faltantes con 0)\n",
        "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    # 8. Entrena el modelo XGBoost con 100 árboles\n",
        "    model = xgb.XGBRegressor(n_estimators=100)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 9. Predice los valores faltantes en la variable objetivo\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # 10. Asigna los valores imputados a las posiciones originales con missing\n",
        "    df.loc[df[target_col].isnull(), target_col] = preds\n",
        "\n",
        "    # 11. Devuelve el DataFrame imputado\n",
        "    return df\n",
        "\n",
        "# Aplicamos la función para imputar los valores faltantes en la variable \"LoanAmount\"\n",
        "df_xgb = xgb_impute(df_xgb, \"LoanAmount\")\n"
      ],
      "metadata": {
        "id": "bg-F6wCNba5K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Imputación MICE (Multivariate Imputation)"
      ],
      "metadata": {
        "id": "Wu8e7VBcbc27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos IterativeImputer desde fancyimpute (implementa el algoritmo MICE)\n",
        "from fancyimpute import IterativeImputer\n"
      ],
      "metadata": {
        "id": "tUOgnm10zwqT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_mice = df.copy()"
      ],
      "metadata": {
        "id": "7k6byNLKzxZF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aplicamos LabelEncoder a todas las columnas para convertir variables categóricas a numéricas\n",
        "# Esto es necesario porque MICE solo puede trabajar con variables numéricas\n",
        "df_mice = df_mice.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Creamos una instancia del imputador MICE (IterativeImputer)\n",
        "# Este imputador usa regresiones iterativas entre variables para predecir valores faltantes\n",
        "mice = IterativeImputer()\n",
        "\n",
        "# Aplicamos el imputador a los datos\n",
        "# fit_transform entrena los modelos internamente y realiza la imputación\n",
        "df_mice_imputed = pd.DataFrame(mice.fit_transform(df_mice), columns=df_mice.columns)\n"
      ],
      "metadata": {
        "id": "Tf_DU7a4bfZw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Autoencoders para imputación"
      ],
      "metadata": {
        "id": "rUCnwkphbhF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos TensorFlow y los componentes necesarios para construir una red neuronal (autoencoder)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_auto = df.copy()\n",
        "\n",
        "# Convertimos todas las variables categóricas a numéricas con LabelEncoder\n",
        "df_auto = df_auto.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Imputamos valores faltantes inicialmente con la media de cada columna\n",
        "# Esto es necesario porque las redes neuronales no pueden recibir NaNs como entrada\n",
        "df_auto = df_auto.fillna(df_auto.mean())\n",
        "\n",
        "# Convertimos el DataFrame a un array de NumPy para entrenar la red\n",
        "X = df_auto.values\n"
      ],
      "metadata": {
        "id": "vt8RT8IBbiuR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la arquitectura del Autoencoder (una red neuronal simétrica)\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X.shape[1],)),  # Capa de entrada → 32 neuronas\n",
        "    Dense(16, activation='relu'),                              # Capa oculta comprimida (bottleneck)\n",
        "    Dense(32, activation='relu'),                              # Capa de expansión (simétrica)\n",
        "    Dense(X.shape[1])                                          # Capa de salida (reconstrucción completa)\n",
        "])\n"
      ],
      "metadata": {
        "id": "-CExkHDV0ZJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilamos el modelo usando el optimizador Adam y la pérdida de error cuadrático medio (MSE)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Entrenamos el modelo para que aprenda a reconstruir sus propias entradas\n",
        "# El autoencoder aprende patrones en los datos para luego usarlos en la imputación\n",
        "model.fit(X, X, epochs=100, batch_size=16, verbose=0)\n"
      ],
      "metadata": {
        "id": "sFiqNCRo0auL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el modelo entrenado para predecir la versión reconstruida de los datos\n",
        "# Esta salida será utilizada como imputación final\n",
        "X_pred = model.predict(X)\n",
        "\n",
        "# Convertimos la salida a un DataFrame con los mismos nombres de columnas que el original\n",
        "df_auto_imputed = pd.DataFrame(X_pred, columns=df_auto.columns)\n"
      ],
      "metadata": {
        "id": "_PqMGgWr0cq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Evaluación del impacto predictivo\n",
        "\n",
        "Utilizaremos un modelo de clasificación (XGBClassifier) para predecir Loan_Status con los diferentes datasets imputados."
      ],
      "metadata": {
        "id": "LdSdwVFKbm0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos herramientas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "KAmNaCV7brsS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una función que evalúa un DataFrame ya imputado\n",
        "# Entrena un modelo para predecir \"Loan_Status\" y calcula métricas de desempeño\n",
        "def evaluate_model(df, name=\"\"):\n",
        "    df = df.copy()  # Hacemos copia para no alterar el original\n",
        "\n",
        "    # Eliminamos cualquier fila que aún tenga valores faltantes\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Separamos variables predictoras (X) de la variable objetivo (y)\n",
        "    X = df.drop(\"Loan_Status\", axis=1)\n",
        "    y = df[\"Loan_Status\"]\n",
        "\n",
        "    # Si la variable objetivo es categórica (tipo object), la codificamos numéricamente\n",
        "    if y.dtype == 'O':\n",
        "        y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # Convertimos todas las variables categóricas (en X) a variables dummy (One-Hot Encoding)\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "    # Dividimos los datos en entrenamiento (80%) y prueba (20%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Creamos y entrenamos el clasificador XGBoost\n",
        "    # Desactivamos use_label_encoder y usamos logloss como métrica\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Generamos predicciones de clase y de probabilidad\n",
        "    preds = model.predict(X_test)\n",
        "    probas = model.predict_proba(X_test)[:,1]  # Probabilidad de clase positiva\n",
        "\n",
        "    # Mostramos métricas: Accuracy y AUC\n",
        "    print(f\"Evaluación con {name}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
        "    print(f\"AUC: {roc_auc_score(y_test, probas):.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "TyVgRBSG1BkM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el desempeño del modelo con diferentes versiones del dataset imputado\n",
        "evaluate_model(df_mean, \"Media/Moda\")               # Imputación simple\n",
        "evaluate_model(df_reg, \"Regresión\")                 # Imputación por regresión lineal\n",
        "evaluate_model(df_knn_imputed, \"KNN\")               # Imputación con K-Nearest Neighbors\n",
        "evaluate_model(df_xgb, \"XGBoost\")                   # Imputación supervisada con XGBoost\n",
        "evaluate_model(df_mice_imputed, \"MICE\")             # Imputación iterativa por MICE\n",
        "df_auto_imputed[\"Loan_Status\"] = df[\"Loan_Status\"]\n",
        "evaluate_model(df_auto_imputed, \"Autoencoder\")      # Imputación usando autoencoders\n"
      ],
      "metadata": {
        "id": "g1CvOCD11EO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preguntas para reflexión\n",
        "\n",
        "- ¿Qué técnica ofrece mejor balance entre simplicidad y precisión?\n",
        "\n",
        "- ¿Qué riesgos podría implicar usar una técnica muy compleja como Autoencoders o XGBoost para imputar?\n",
        "\n",
        "- ¿Por qué KNN puede ser sensible a la escala de los datos o a outliers?\n",
        "\n",
        "- ¿Cómo se podría incorporar la incertidumbre de la imputación en el modelo final?\n",
        "\n",
        "- ¿Qué tipo de datos (categóricos, numéricos, multivariados) favorecen el uso de MICE sobre otros métodos?"
      ],
      "metadata": {
        "id": "vDWnUyhrb44e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusión\n",
        "\n",
        "La imputación no es solo un paso técnico: es una decisión analítica que puede alterar el resultado del modelo. Evaluar diferentes métodos no solo mejora el desempeño, sino también la confianza en los modelos desarrollados. Entender cuándo usar cada técnica y su impacto es clave para un análisis responsable."
      ],
      "metadata": {
        "id": "QVfOuKovb-94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Gracias por completar este laboratorio!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cDjdWiDRcDSI"
      }
    }
  ]
}